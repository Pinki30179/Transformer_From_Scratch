{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0fEQbhdmpY5U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eca8e4d7-6138-4861-f274-f4163f3a23ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!‘9m#.ïFé-pvB[JJ2,[k/?Uï,ñ$ñzp!wx4AFHcQ‘D7i-XPr#!_‘$KP•-j/Wp.ZNX/[U4XXP%r!ñ\n",
            "3Mu!Cq(p-QNWPIo2%-NH\"zwK\n",
            "Epoch : 1, Loss : 4.6961989402771\n",
            "Epoch : 2, Loss : 5.4317946434021\n",
            "Epoch : 3, Loss : 5.201897621154785\n",
            "Epoch : 4, Loss : 5.288149833679199\n",
            "Epoch : 5, Loss : 5.189873695373535\n",
            "Epoch : 6, Loss : 4.911618709564209\n",
            "Epoch : 7, Loss : 4.793488502502441\n",
            "Epoch : 8, Loss : 4.772161483764648\n",
            "Epoch : 9, Loss : 4.815664768218994\n",
            "Epoch : 10, Loss : 4.785419940948486\n",
            "Epoch : 11, Loss : 4.7387824058532715\n",
            "Epoch : 12, Loss : 4.690345287322998\n",
            "Epoch : 13, Loss : 4.673628807067871\n",
            "Epoch : 14, Loss : 4.6582489013671875\n",
            "Epoch : 15, Loss : 4.633567810058594\n",
            "tensor([[ 2, 86, 37, 45,  8, 70, 83, 41, 26, 83, 81, 42, 79, 41, 93, 45, 78, 81,\n",
            "         42, 19, 19, 45, 43, 80, 19, 79, 56, 18,  5, 57, 56, 85, 17,  8, 64, 18,\n",
            "         45, 30,  7, 54, 10, 15, 12, 74, 56, 47, 76, 61, 82, 52, 16, 90, 16, 91,\n",
            "         57, 44, 61, 22, 21, 43,  7,  1,  9, 83, 89, 15, 77, 74, 14, 66, 42, 90,\n",
            "         43, 19, 15, 90, 73, 65,  9, 88, 65, 88, 29, 91, 14, 17, 93, 27, 74, 56,\n",
            "         66, 88, 39, 56, 17, 86, 93, 60, 88, 66]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy\n",
        "\n",
        "from numpy.ma.core import indices\n",
        "from torch.ao.nn.quantized import ReLU6\n",
        "from torch.nn import ReLU\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dimension_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dimension_head = d_model // num_heads\n",
        "\n",
        "        self.weight_query = nn.Linear(d_model, d_model)\n",
        "        self.weight_key = nn.Linear(d_model, d_model)\n",
        "        self.weight_value = nn.Linear(d_model, d_model)\n",
        "        self.weight_output = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, query, key, value, mask = None):\n",
        "        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.dimension_head)\n",
        "        if mask is not None:\n",
        "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
        "        attention_probs = torch.softmax(attention_scores, dim = -1)\n",
        "        output = torch.matmul(attention_probs, value)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.dimension_head).transpose(1,2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1,2).contiguous().view(batch_size, seq_length, self.dimension_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        query = self.split_heads(self.weight_query(query))\n",
        "        key = self.split_heads(self.weight_key(key))\n",
        "        value = self.split_heads(self.weight_value(value))\n",
        "\n",
        "        attention_output = self.scaled_dot_product_attention(query, key, value, mask)\n",
        "        output = self.weight_output(self.combine_heads(attention_output))\n",
        "        return output\n",
        "\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "       super().__init__()\n",
        "\n",
        "       pe = torch.zeros(max_seq_length, d_model)\n",
        "       position = torch.arange(0, max_seq_length, dtype = torch.float).unsqueeze(1)\n",
        "       div_term = torch.exp(torch.arange(0, d_model, 2).float()* -(math.log(10000.0)/d_model))\n",
        "\n",
        "       pe[:, ::2] = torch.sin(position*div_term)\n",
        "       pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "       self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_out, src_mask, tgt_mask):\n",
        "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        attn_output = self.cross_attn(x, enc_out, enc_out, src_mask)\n",
        "        x = self.norm2(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super().__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList(\n",
        "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList(\n",
        "            [DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)\n",
        "        seq_length = tgt.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(seq_length, seq_length), diagonal=1)).bool()\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        output = self.fc(dec_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "    def generate(self, src, start_token, max_length, temperature=1.0, top_k=None):\n",
        "        self.eval()\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        generated = torch.tensor([[start_token]], dtype=torch.long)\n",
        "\n",
        "        for _ in range(max_length - 1):\n",
        "            tgt_mask = (generated != 0).unsqueeze(1).unsqueeze(2)\n",
        "            seq_length = generated.size(1)\n",
        "            nopeak_mask = (1 - torch.triu(torch.ones(seq_length, seq_length), diagonal=1)).bool()\n",
        "            tgt_mask = tgt_mask & nopeak_mask\n",
        "\n",
        "            tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(generated)))\n",
        "            dec_output = tgt_embedded\n",
        "            for dec_layer in self.decoder_layers:\n",
        "                dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "            logits = self.fc(dec_output[:, -1, :]) / temperature\n",
        "            if top_k is not None:\n",
        "                top_k = min(top_k, logits.size(-1))\n",
        "                values, indices = torch.topk(logits, top_k)\n",
        "                logits = torch.full_like(logits, float('-inf')).scatter(-1, indices, values)\n",
        "\n",
        "            probs = nn.functional.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            generated = torch.cat([generated, next_token], dim=1)\n",
        "\n",
        "            if len(generated[0]) == max_length:\n",
        "                break\n",
        "        return generated\n",
        "\n",
        "\n",
        "with open('/content/input.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "v_size = len(chars)\n",
        "\n",
        "src_vocab_size = v_size\n",
        "tgt_vocab_size = v_size\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "max_seq_length =100\n",
        "dropout = 0.1\n",
        "\n",
        "string_to_int = {c: i for i, c in enumerate(chars)}\n",
        "int_to_string = {i: c for i, c in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [string_to_int[c] for c in s]\n",
        "decode = lambda i: ''.join(int_to_string[c.item()] for c in i)\n",
        "\n",
        "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
        "context_with_given_input = torch.tensor([encode('hello')], dtype = torch.long)\n",
        "src = torch.tensor([[1]])\n",
        "generated = transformer.generate(\n",
        "    src = context_with_given_input,\n",
        "    start_token = 2,\n",
        "    max_length = max_seq_length,\n",
        "    temperature = 0.7,\n",
        "    top_k = None\n",
        ")\n",
        "print(decode(generated[0]))\n",
        "\n",
        "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))\n",
        "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(transformer.parameters(), lr=0.0065, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer.train()\n",
        "\n",
        "for epoch in range(15):\n",
        "    optimizer.zero_grad()\n",
        "    output = transformer(src_data, tgt_data[:, :-1])\n",
        "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:,1:].contiguous().view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch : {epoch+1}, Loss : {loss.item()}\")\n",
        "\n",
        "generated = transformer.generate(\n",
        "    src=src,\n",
        "    start_token =2,\n",
        "    max_length= max_seq_length,\n",
        "    temperature = 0.7,\n",
        "    top_k = None\n",
        ")\n",
        "print(generated)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_output = decode(generated[0])\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZeN3Ib-rWZy",
        "outputId": "8cca71ba-c84a-48c0-a234-9dd75524cb62"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!ïIQ'mzM:zxNvM•QuxN33QOw3v]2$_]é1'g2QB&Z)/,q]SsdyX0’0“_Pd65O& (z‘/tq.iN’O3/’ph(—h—A“.1•;q]i—K]1ï•c—i\n"
          ]
        }
      ]
    }
  ]
}